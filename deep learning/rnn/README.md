# RNN

传统的神经网络处理自然语言问题的缺点：

- 不同实例输入和输出的长度可能是不一样的。
- 不能学习到词语之间的序列（位置）信息。

![](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)

### 计算步骤

- $x_t$是时间$t$的输入。
- $s_t$是隐层在时间t的状态值。该值也称为“**记忆**”，它的值取决于$s_{t-1}$和$x_t$，即$s_t=f(Ux_t+Ws_{t-1})$，这个激活函数通常选择**tanh**和**ReLU**。$s_{-1}$通常初始化为0。
- $o_t$是我们在时间t的输出，它通常是在词汇表上的softmax：$o_t = softmax(Vs_t)$。

### 特性

- 网络结构有$U,V,W$三个参数，这三个参数在每个时刻共享。
- 没有用到后面的信息，只用到了前面的信息。
- Elman Network 上一个时间点的隐层输出到下一个时间点
- Jordan Network 上一个时间点的输出层接到下一个时间点

### 类型

![](./images/3.png)

- **Many-to-One** 输入是多个，但是输出只要求是最后一个时刻的值，比如做情感分析，只需在看完整个评论语句后再输出结果即可。
- **Many-to-Many** 输入和输出都是多个，但是输入和输出的长度可能不一样，如机器翻译。但是输入输出也可能是一样的，如**命名体识别**。
- **One-to-Many** 输入是单个，而输出可以是多个，音乐生成。

### 应用

- **语言建模和文本生成**
  - 给定一个文本，我们想根据之前的一些单词预测后面出现的单词。语言模型可以给出该句子的概率。
  - 具体的做法的话， 我们可以在输出的时候，对$\hat{y}^{<i>}$根据其sotfmax值进行采样。
- **机器翻译**
  - ![](http://www.wildml.com/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-10.39.06-AM-1024x557.png)
- **语音识别**
- **生成图片描述**

### 训练

RNN的训练方式也是通过反向传播，但是和传统神经网络训练过程不同的是，我们在计算当前时刻的梯度的时候，需要考虑之前时刻的值。例如，t=4，我们需要反向传播3步，然后把所有的gradient加起来。这种方式成为**Backpropagation Through Time(BPTT)。**但是，在RNN训练的过程中，会出现**梯度消失**和**梯度爆炸**的问题。

## Bidirectional RNNs

## Deep(Bidirectional) RNNs

